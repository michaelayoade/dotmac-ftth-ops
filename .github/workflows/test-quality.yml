# Test Quality CI/CD Pipeline
#
# This workflow runs comprehensive test quality checks including:
# - Test pyramid marker coverage tracking
# - Mock usage reduction tracking
# - Parallel execution readiness monitoring
# - Schema validation
# - OpenAPI contract testing
# - Performance regression detection
# - Coverage reporting

name: Test Quality

on:
  pull_request:
    branches: [main, develop]
  push:
    branches: [main, develop]
  schedule:
    # Run nightly for performance tracking
    - cron: '0 2 * * *'

jobs:
  test-metrics:
    name: Test Metrics & Quality Gates
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install Poetry
        run: |
          curl -sSL https://install.python-poetry.org | python3 -
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: poetry install --with dev

      - name: Track test pyramid progress
        id: pyramid
        run: |
          echo "üìä Test Pyramid Progress"
          python3 scripts/test_pyramid_progress.py --json > pyramid-progress.json

          # Extract key metrics
          UNMARKED=$(python3 -c "import json; data=json.load(open('pyramid-progress.json')); print(sum(m['stats']['unmarked_files'] for m in data.values()))")
          TOTAL=$(python3 -c "import json; data=json.load(open('pyramid-progress.json')); print(sum(m['stats']['total_files'] for m in data.values()))")

          echo "unmarked=$UNMARKED" >> $GITHUB_OUTPUT
          echo "total=$TOTAL" >> $GITHUB_OUTPUT

          # Show summary
          python3 scripts/test_pyramid_progress.py

      - name: Fail if unmarked tests detected
        if: steps.pyramid.outputs.unmarked != '0'
        run: |
          echo "‚ùå Found ${{ steps.pyramid.outputs.unmarked }} unmarked test files!"
          echo "All test files must have @pytest.mark.unit, @pytest.mark.integration, or @pytest.mark.e2e markers"
          echo ""
          echo "To fix, add markers to the files listed in pyramid-progress.json"
          echo "Example: pytestmark = pytest.mark.integration"
          exit 1

      - name: Track mock usage
        id: mocks
        run: |
          echo "üîç Mock Usage Tracking"
          bash scripts/count_mocks.sh > mock-report.txt
          cat mock-report.txt

          # Extract total mock usage
          MOCK_TOTAL=$(grep -oP 'üìä Total mock import/usage lines: \K\d+' mock-report.txt || echo "0")
          echo "total=$MOCK_TOTAL" >> $GITHUB_OUTPUT

          # Store baseline for comparison
          echo "$MOCK_TOTAL" > mock-baseline.txt

      - name: Check for mock usage increase
        if: github.event_name == 'pull_request'
        run: |
          # Download baseline from main branch
          git fetch origin main:main || true
          BASELINE=0
          if git show main:mock-baseline.txt &>/dev/null; then
            BASELINE=$(git show main:mock-baseline.txt)
          fi

          CURRENT=${{ steps.mocks.outputs.total }}

          if [ "$CURRENT" -gt "$BASELINE" ]; then
            echo "‚ö†Ô∏è  Warning: Mock usage increased from $BASELINE to $CURRENT lines"
            echo "Consider using factories or fakes instead of mocks"
            echo "See tests/billing/factories.py for examples"
          else
            echo "‚úÖ Mock usage: $CURRENT lines (baseline: $BASELINE)"
          fi

      - name: Check parallel execution markers
        id: parallel
        run: |
          echo "‚ö° Parallel Execution Readiness"
          SERIAL=$(rg -g 'test_*.py' -o 'pytest\.mark\.serial_only' tests | wc -l | tr -d ' ')
          PARALLEL=$(rg -g 'test_*.py' -o 'pytest\.mark\.parallel_safe' tests | wc -l | tr -d ' ')

          echo "serial_only=$SERIAL" >> $GITHUB_OUTPUT
          echo "parallel_safe=$PARALLEL" >> $GITHUB_OUTPUT

          echo "Serial-only tests: $SERIAL"
          echo "Parallel-safe tests: $PARALLEL"

          if [ "$PARALLEL" -lt 20 ]; then
            echo "‚ö†Ô∏è  Only $PARALLEL tests marked as parallel_safe"
            echo "Consider marking more integration tests as parallel_safe to speed up CI"
          fi

      - name: Upload metrics artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-metrics
          path: |
            pyramid-progress.json
            mock-report.txt
            mock-baseline.txt
          retention-days: 30

      - name: Comment PR with metrics
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');

            let pyramidData = {};
            try {
              pyramidData = JSON.parse(fs.readFileSync('pyramid-progress.json', 'utf8'));
            } catch (e) {
              console.log('No pyramid data found');
            }

            const mockReport = fs.readFileSync('mock-report.txt', 'utf8').split('\n');
            const mockTotal = mockReport.find(l => l.includes('Total mock import/usage lines'))?.match(/\d+/)?.[0] || '0';

            const unmarked = ${{ steps.pyramid.outputs.unmarked }};
            const total = ${{ steps.pyramid.outputs.total }};
            const serial = ${{ steps.parallel.outputs.serial_only }};
            const parallel = ${{ steps.parallel.outputs.parallel_safe }};

            let comment = `## üìä Test Quality Metrics\n\n`;

            // Pyramid markers
            comment += `### Test Pyramid Coverage\n`;
            if (unmarked === 0) {
              comment += `‚úÖ All ${total} test files have pyramid markers\n\n`;
            } else {
              comment += `‚ö†Ô∏è  ${unmarked} of ${total} test files missing markers\n`;
              comment += `Please add \`pytestmark = pytest.mark.unit/integration/e2e\`\n\n`;
            }

            // Mock usage
            comment += `### Mock Usage\n`;
            comment += `üì¶ Current: **${mockTotal} lines** of mock-related code\n`;
            comment += `üéØ Goal: Reduce by adopting factories (see \`tests/billing/factories.py\`)\n\n`;

            // Parallel readiness
            comment += `### Parallel Execution\n`;
            comment += `‚ö° Serial-only: ${serial} tests\n`;
            comment += `üöÄ Parallel-safe: ${parallel} tests\n`;
            if (parallel < 20) {
              comment += `\nüí° Tip: Mark more integration tests as \`@pytest.mark.parallel_safe\`\n`;
            }
            comment += `\n---\n`;
            comment += `üìñ See [unit-test-review-verified.md](../docs/unit-test-review-verified.md) for detailed guidance\n`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  test-quality:
    name: Schema Validation & Coverage
    runs-on: ubuntu-latest
    needs: test-metrics

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[test]"

      - name: Run tests with coverage
        run: |
          pytest tests/ --cov=src --cov-report=json --cov-report=html --cov-report=term -v

      - name: Check schema validation
        run: |
          # Run contract testing validation
          python scripts/validate_test_schemas.py
        continue-on-error: true

      - name: Validate OpenAPI contracts
        if: hashFiles('openapi.json') != ''
        run: |
          python scripts/validate_openapi_contracts.py
        continue-on-error: true

      - name: Check performance regression
        run: |
          python scripts/check_performance_regression.py --threshold 20
        continue-on-error: true

      - name: Generate coverage dashboard
        run: |
          python scripts/generate_coverage_dashboard.py
        continue-on-error: true

      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.json
          fail_ci_if_error: false

      - name: Upload performance data
        uses: actions/upload-artifact@v4
        with:
          name: performance-data
          path: .test_performance.json

      - name: Upload coverage dashboard
        uses: actions/upload-artifact@v4
        with:
          name: coverage-dashboard
          path: coverage-dashboard.html

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');

            // Read performance data
            let perfData = {};
            try {
              perfData = JSON.parse(fs.readFileSync('.test_performance.json', 'utf8'));
            } catch (e) {
              console.log('No performance data found');
            }

            // Read coverage data
            let covData = {};
            try {
              covData = JSON.parse(fs.readFileSync('coverage.json', 'utf8'));
            } catch (e) {
              console.log('No coverage data found');
            }

            const coverage = covData.totals?.percent_covered || 0;
            const slowTests = Object.entries(perfData.tests || {})
              .sort((a, b) => b[1].avg_ms - a[1].avg_ms)
              .slice(0, 5);

            let comment = `## Test Coverage & Performance\n\n`;
            comment += `### Coverage\n`;
            comment += `**${coverage.toFixed(1)}%** test coverage\n\n`;

            if (slowTests.length > 0) {
              comment += `### Performance\n`;
              comment += `Slowest 5 tests:\n`;
              for (const [name, stats] of slowTests) {
                comment += `- ${name}: ${stats.avg_ms.toFixed(0)}ms\n`;
              }
            }

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  performance-regression:
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request'

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Download previous performance data
        uses: dawidd6/action-download-artifact@v2
        continue-on-error: true
        with:
          workflow: test-quality.yml
          name: performance-data
          path: ./previous

      - name: Run performance comparison
        run: |
          if [ -f previous/.test_performance.json ]; then
            python scripts/check_performance_regression.py \
              --previous previous/.test_performance.json \
              --current .test_performance.json \
              --threshold 20 \
              --report
          fi
